Lecture 4/1/2025 
- this one is from Slides 9 onward

- Heap will be on this tests
- Starting from Heap to Minimum Spanning Tree

### üî∫ What is a Heap?

A **heap** is a special type of **binary tree** that satisfies the following properties:

1. **Complete Binary Tree**: Every level is fully filled except possibly the last, which is filled from left to right.
2. **Heap Property**: The value of each node is **ordered with respect to its children**.

There are two main types:

### ‚úÖ Min-Heap

- **Definition**: A binary heap where **each parent node is less than or equal to its children**.
- **Property**: The **minimum element is always at the root**.
- **Use Case**: Priority queues (where the smallest value gets removed first).

```
        2
      /   \
     4     5
    / \   / 
   9  10 8
```

---

### ‚úÖ Max-Heap

- **Definition**: A binary heap where **each parent node is greater than or equal to its children**.
- **Property**: The **maximum element is always at the root**.
- **Use Case**: Priority queues where highest priority = largest number.

```
        10
       /  \
      9    5
     / \   /
    2   4 3
```

---

### üì¶ Array Representation of a Heap

We store the heap as an array where:

- **Root is at index 0** (or index 1 depending on convention).
- For a node at index `i`:
  - **Left child** = `2*i + 1`
  - **Right child** = `2*i + 2`
  - **Parent** = `(i - 1) / 2` (integer division)

**Example Min-Heap stored as array:**

```
Heap Tree:
        2
      /   \
     4     5
    / \   /
   9 10  8

Array: [2, 4, 5, 9, 10, 8]
```

---

### ‚è¨ Heap Operations (Min-Heap)

- **Insert**:
  - Add at the end of array, then **bubble up** (swap with parent until heap property is restored).
- **Extract Min**:
  - Remove root (index 0), move last element to root, then **heapify down** (swap with smaller child until heap property restored).
- **Build Heap**:
  - From unsorted array in O(n) time using `heapify()` from bottom-up.

#### Adding & Delting Nodes
- New nodes are always inserted at the bottom level (Left to right)
- Nodes are removed from the bottom level (right to left)

- Max-Heap-Insert Algorithm
Alg(MAX-HEAP-INSERT(A, key, n)
1. heap-size[A] <- n + 1
2. A[n + 1] <- -infinity
3. HEAP-INCREASE-KEY(A, n + 1, key)
-- running time is O(log n)



---

### üå≥ What is a Binary Search Tree (BST)?

A **Binary Search Tree** is a **binary tree** with the following properties:

1. **Left Subtree < Root**: All nodes in the left subtree have **values less than the root**.
2. **Right Subtree > Root**: All nodes in the right subtree have **values greater than the root**.
3. **No duplicates** (in standard BSTs).
4. This rule applies **recursively** to every subtree.

---

### üìà Example of a BST

```
        8
       / \
      3   10
     / \    \
    1   6    14
       / \   /
      4   7 13
```

---

### üß† Key BST Operations

| Operation     | Description                                      | Time (avg) | Time (worst) |
|---------------|--------------------------------------------------|------------|---------------|
| **Insert**     | Add a value by traversing left/right.            | O(log n)   | O(n)         |
| **Search**     | Look for a value via comparisons.                | O(log n)   | O(n)         |
| **Delete**     | Remove a node, handle 3 cases (see below).       | O(log n)   | O(n)         |
| **Traversal**  | In-order (sorted), Pre-order, Post-order, etc.   | O(n)       | O(n)         |

---

### üóëÔ∏è Deletion in BST (3 Cases)

1. **Node is a leaf**: Just delete it.
2. **Node has one child**: Replace it with its child.
3. **Node has two children**:
   - Find **in-order successor** (smallest in right subtree) or **in-order predecessor** (largest in left subtree).
   - Replace node‚Äôs value with that.
   - Delete successor/predecessor node.

---

### üîÅ BST Traversals

- **In-order (LNR)**: Left ‚Üí Node ‚Üí Right ‚Üí gives **sorted order**.
- **Pre-order (NLR)**: Node ‚Üí Left ‚Üí Right ‚Üí useful for copying the tree.
- **Post-order (LRN)**: Left ‚Üí Right ‚Üí Node ‚Üí used to delete the tree.

---

### ‚ùóLimitations

- Not balanced ‚Üí can degrade to **linked list** (e.g., inserting sorted elements).
- Worst case time: O(n)
  
‚úÖ That‚Äôs why we have **balanced BSTs** like **AVL Tree** or **Red-Black Tree** (which we'll cover next if you want).

Awesome ‚Äî let‚Äôs talk about **balancing a Binary Search Tree (BST)**, which is crucial for keeping operations fast. Here's your quick, clear breakdown:

---

### ‚öñÔ∏è Why Balance a BST?

- **Unbalanced BST**: If you insert elements in sorted order (e.g., 1, 2, 3, 4‚Ä¶), the BST becomes a **linked list**:
  ```
  1
   \
    2
     \
      3
       \
        4
  ```
  - Operations like search, insert, and delete degrade from **O(log n)** to **O(n)**.

- **Balanced BST**: Keeps the tree‚Äôs height as small as possible, ideally **O(log n)**. This ensures operations remain efficient.

---

### üõ†Ô∏è Types of Balanced Trees

#### 1. **AVL Tree**
- **Self-balancing BST**: After each insert/delete, it checks balance.
- **Balance factor** = height(left subtree) ‚Äì height(right subtree)
  - Must be -1, 0, or +1.
- If not, perform **rotations** (single or double) to rebalance.

‚úÖ Rotations:
- **Right Rotation (RR)**: Used when left-left heavy.
- **Left Rotation (LL)**: Used when right-right heavy.
- **Left-Right (LR)** and **Right-Left (RL)**: Two-step rotations.

#### 2. **Red-Black Tree**
- A BST where each node is **red or black**, with rules to maintain balance.
- Ensures the **longest path is no more than twice the shortest**.
- Used in many real-world systems (e.g., Java TreeMap, Linux scheduler).

Red-Black Rules:
1. Every node is red or black.
2. Root is always black.
3. No two red nodes in a row.
4. Every path from a node to null must have the same number of black nodes.

üîÑ Insertion/Deletion may trigger **color flips** or **rotations**.

#### 3. **Splay Tree**
- Recently accessed elements are moved to the root using rotations.
- Helps with **temporal locality** (if you access the same data repeatedly).

---

### ‚è≥ Summary of Time Complexities

| Tree Type     | Insert | Search | Delete |
|---------------|--------|--------|--------|
| BST (avg)     | O(log n) | O(log n) | O(log n) |
| BST (worst)   | O(n)     | O(n)     | O(n)     |
| AVL Tree      | O(log n) | O(log n) | O(log n) |
| Red-Black Tree| O(log n) | O(log n) | O(log n) |

---
Examples of Rotation: 

We‚Äôll do:

1. **Right Rotation (LL case)**  
2. **Left Rotation (RR case)**  
3. **Left-Right Rotation (LR case)**  
4. **Right-Left Rotation (RL case)**

---

### 1Ô∏è‚É£ Right Rotation (LL Case)

**Trigger**: Inserting into the **left subtree of the left child**.

#### Insert: 30 ‚Üí 20 ‚Üí 10

```
Before rotation:
        30
       /
     20
    /
  10

Balance factor at 30 = 2 (left-heavy)
```

**Right Rotation around 30:**

```
After rotation:
      20
     /  \
   10    30
```

---

### 2Ô∏è‚É£ Left Rotation (RR Case)

**Trigger**: Inserting into the **right subtree of the right child**.

#### Insert: 10 ‚Üí 20 ‚Üí 30

```
Before rotation:
    10
      \
       20
         \
          30

Balance factor at 10 = -2 (right-heavy)
```

**Left Rotation around 10:**

```
After rotation:
       20
      /  \
    10    30
```

---

### 3Ô∏è‚É£ Left-Right Rotation (LR Case)

**Trigger**: Inserting into the **right subtree of the left child**.

#### Insert: 30 ‚Üí 10 ‚Üí 20

```
Before rotation:
        30
       /
     10
       \
        20

Balance factor at 30 = 2 (left-heavy), but left child is right-heavy.
```

**Step 1: Left rotation at 10**

```
        30
       /
     20
     /
   10
```

**Step 2: Right rotation at 30**

```
      20
     /  \
   10    30
```

---

### 4Ô∏è‚É£ Right-Left Rotation (RL Case)

**Trigger**: Inserting into the **left subtree of the right child**.

#### Insert: 10 ‚Üí 30 ‚Üí 20

```
Before rotation:
    10
      \
       30
      /
    20

Balance factor at 10 = -2 (right-heavy), but right child is left-heavy.
```

**Step 1: Right rotation at 30**

```
    10
      \
       20
         \
          30
```

**Step 2: Left rotation at 10**

```
      20
     /  \
   10    30
```

---


---

### üîë What is Hashing?

**Hashing** is the process of converting a key into an index using a **hash function**, so data can be inserted, searched, or deleted in near **constant time** (O(1) average case).

---

### üß† Hash Table

- **Data structure** that stores key-value pairs.
- Uses a **hash function** to compute an index in an array (the table).
- Ideal for quick lookups like dictionaries, symbol tables, caches.

---

### ‚öôÔ∏è Hash Function

A **hash function** `h(key)` maps a key to an index in the table.

Requirements:
- Must be **fast**
- Should **distribute keys uniformly**
- Must be **deterministic** (same key ‚Üí same hash)

Example (simple):
```cpp
int hash(int key) {
    return key % table_size;
}
```

---

### ‚ùå Collisions

When two keys hash to the **same index**, it's called a **collision**.

---

### üõ†Ô∏è Collision Resolution Techniques

#### 1. **Chaining (Separate Chaining)**
- Each slot holds a **linked list** of entries.
- When collision occurs, just add to the list at that index.

```
Table[2]: ‚Üí (12) ‚Üí (22)
```

#### 2. **Open Addressing**
- All data is stored directly in the table (no extra structure).
- When a collision happens, probe for the next empty slot.

**Probing methods:**
- **Linear Probing**: `h(k), h(k)+1, h(k)+2, ...`
- **Quadratic Probing**: `h(k), h(k)+1¬≤, h(k)+2¬≤, ...`
- **Double Hashing**: Use second hash function for jump size.

---

### üìà Load Factor (Œ±)

- **Œ± = n / table_size**
- Ratio of stored elements to table size.
- High Œ± ‚áí more collisions.
- Keep **Œ± < 0.7** for good performance (esp. in open addressing).

---

### üîÅ Rehashing

- When load factor gets too high, create a new larger table and re-insert all keys.
- Often double the size (next prime is ideal).

---

### ‚úÖ Hash Table Time Complexity

| Operation | Avg Time | Worst Time |
|-----------|----------|------------|
| Insert    | O(1)     | O(n)       |
| Search    | O(1)     | O(n)       |
| Delete    | O(1)     | O(n)       |

(Avg case assumes good hash function and low collisions.)

---

### üß™ Example:

```cpp
table_size = 7
keys = [10, 20, 15, 7]
hash(key) = key % 7

Indexes:
10 % 7 = 3 ‚Üí table[3]
20 % 7 = 6 ‚Üí table[6]
15 % 7 = 1 ‚Üí table[1]
7  % 7 = 0 ‚Üí table[0]
```

---

Absolutely! Let‚Äôs look at **examples** of the three main **open addressing** methods in hash tables:

We'll use:
- **Hash function**: `h(key) = key % 7`
- **Table size**: `7`
- We'll insert: `10, 20, 15, 7, 32`

---

### üì¶ Initial Setup:

```plaintext
Empty table (size = 7):
[0] _
[1] _
[2] _
[3] _
[4] _
[5] _
[6] _
```

---

### üîÅ 1. **Linear Probing**

If collision occurs, probe **next slot (i + 1)** until empty.

**Insert 10** ‚Üí 10 % 7 = 3  
‚Üí table[3] is empty ‚Üí insert at index 3

**Insert 20** ‚Üí 20 % 7 = 6  
‚Üí table[6] is empty ‚Üí insert at index 6

**Insert 15** ‚Üí 15 % 7 = 1  
‚Üí table[1] is empty ‚Üí insert at index 1

**Insert 7** ‚Üí 7 % 7 = 0  
‚Üí table[0] is empty ‚Üí insert at index 0

**Insert 32** ‚Üí 32 % 7 = 4  
‚Üí table[4] is empty ‚Üí insert at index 4

‚úÖ Final Table:
```plaintext
[0]  7
[1] 15
[2] _
[3] 10
[4] 32
[5] _
[6] 20
```

Now let's see what happens **with a collision**:

**Insert 17** ‚Üí 17 % 7 = 3 ‚Üí table[3] is full  
Linear probing checks:
- table[4] ‚Üí full  
- table[5] ‚Üí empty ‚Üí insert at 5

‚úÖ Table now:
```plaintext
[0]  7
[1] 15
[2] _
[3] 10
[4] 32
[5] 17
[6] 20
```

---

### üîÅ 2. **Quadratic Probing**

Probes in steps of `i¬≤`:  
index = `(h(key) + i¬≤) % table_size`, for i = 0, 1, 2...

**Insert 10** ‚Üí 10 % 7 = 3 ‚Üí table[3] = empty ‚Üí insert

**Insert 17** ‚Üí 17 % 7 = 3 ‚Üí collision  
Try:
- i=1 ‚Üí (3 + 1¬≤) % 7 = 4 ‚Üí empty ‚Üí insert at index 4

**Insert 24** ‚Üí 24 % 7 = 3 ‚Üí collision  
Try:
- i=1 ‚Üí (3 + 1¬≤) % 7 = 4 ‚Üí full  
- i=2 ‚Üí (3 + 4) % 7 = 0 ‚Üí insert at index 0

‚úÖ Table:
```plaintext
[0] 24
[1] _
[2] _
[3] 10
[4] 17
[5] _
[6] _
```

---

### üîÅ 3. **Double Hashing**

Use two hash functions:
- `h1(key) = key % 7`
- `h2(key) = 5 - (key % 5)` (secondary hash must not be 0!)

Probe: `(h1 + i * h2) % table_size`

**Insert 10**:  
- h1 = 10 % 7 = 3  
- h2 = 5 - (10 % 5) = 5 - 0 = 5  
‚Üí table[3] = empty ‚Üí insert

**Insert 17**:  
- h1 = 3, h2 = 3  
- index = (3 + 0√ó3) % 7 = 3 ‚Üí collision  
- i=1 ‚Üí (3 + 1√ó3) % 7 = 6 ‚Üí insert at 6

**Insert 24**:  
- h1 = 3, h2 = 1  
- i=0 ‚Üí 3  
- i=1 ‚Üí 4 ‚Üí insert at 4

‚úÖ Table:
```plaintext
[0] _
[1] _
[2] _
[3] 10
[4] 24
[5] _
[6] 17
```

---

### üß† Summary

| Method            | Probe Sequence (on collision)           |
|------------------|------------------------------------------|
| Linear Probing   | h, h+1, h+2, ...                         |
| Quadratic Probing| h, h+1¬≤, h+2¬≤, ...                       |
| Double Hashing   | h1, h1 + h2, h1 + 2*h2, ...              |

Each method has tradeoffs:
- **Linear probing**: Simple, but causes **clustering**.
- **Quadratic probing**: Reduces clustering, but may fail to find empty slot if table is too full.
- **Double hashing**: Best distribution but needs good second hash.

---

Awesome! Let‚Äôs tackle **Graphs** next ‚Äî an essential part of Data Structures and Algorithms. We‚Äôll go step by step: definition ‚ûù BFS ‚ûù DFS ‚ûù comparisons.

---

### üåê What is a Graph?

A **graph** is a collection of:

- **Vertices (nodes)**: Represent entities.
- **Edges (connections)**: Represent relationships between vertices.

Graphs can be:

1. **Directed** or **Undirected**  
   - Directed: edges have a direction (A ‚ûù B)  
   - Undirected: edges go both ways (A ‚Äî B)

2. **Weighted** or **Unweighted**  
   - Weighted: edges have costs/weights (like distance, time, etc.)  
   - Unweighted: all edges treated equally

3. **Cyclic** or **Acyclic**  
   - Cyclic: contains cycles  
   - Acyclic: no cycles (e.g., DAG ‚Äì Directed Acyclic Graph)

4. **Connected** or **Disconnected**  
   - Connected: there's a path between every pair of vertices  
   - Disconnected: not all nodes are reachable from each other

---

### üìä Graph Representations

1. **Adjacency Matrix**  
   - 2D array where `matrix[i][j] = 1` if there‚Äôs an edge from `i` to `j`
   - Pros: Fast lookup  
   - Cons: Space = O(V¬≤)

2. **Adjacency List**  
   - Array of lists: each index `i` stores all neighbors of vertex `i`  
   - Pros: Space = O(V + E), better for sparse graphs

---

### üîç Breadth-First Search (BFS)

- **Level-order traversal** of a graph.
- Uses a **queue (FIFO)**.
- Good for **finding shortest path (unweighted)**, or all nodes reachable from a start node.

#### Pseudocode:
```cpp
BFS(Graph, start):
  create empty queue Q
  mark start as visited
  enqueue start into Q

  while Q is not empty:
    node = Q.dequeue()
    for each neighbor of node:
      if not visited:
        mark visited
        enqueue neighbor
```

#### Example:

Graph:
```
A -- B -- C
|         |
D ------- E
```

**BFS from A** ‚Üí `A, B, D, C, E`

---

### üîé Depth-First Search (DFS)

- **Explore as deep as possible** before backtracking.
- Uses **stack** (explicit or via recursion).
- Good for path-finding, cycle detection, topological sort, etc.

#### Pseudocode (recursive):
```cpp
DFS(Graph, node):
  mark node as visited
  for each neighbor:
    if not visited:
      DFS(neighbor)
```

**DFS from A** ‚Üí `A, B, C, E, D` *(depends on adjacency order)*

---

### üîÅ BFS vs. DFS

| Feature            | BFS                         | DFS                         |
|--------------------|-----------------------------|-----------------------------|
| Uses               | Queue                       | Stack / Recursion           |
| Traversal Order    | Level by level              | Deep before wide            |
| Space              | O(V)                        | O(V)                        |
| Shortest Path      | ‚úÖ Yes (unweighted graphs)   | ‚ùå No                        |
| Cycle Detection    | ‚úÖ (with parent tracking)    | ‚úÖ                          |
| Complete?          | Yes                         | Yes                         |

---

### üìö Common Applications

- **BFS**: Shortest path (unweighted), social networks, web crawlers  
- **DFS**: Topological sort, detecting cycles, solving puzzles (e.g., maze), connected components

---

Awesome ‚Äî you‚Äôre covering some of the most powerful graph algorithms! Let‚Äôs go over:

1. ‚úÖ **Dijkstra‚Äôs Algorithm** (for shortest paths)  
2. ‚úÖ **Minimum Spanning Tree (MST)**  
3. ‚úÖ **Prim‚Äôs Algorithm** (to find MST)

---

## üöÄ Dijkstra‚Äôs Algorithm

**Purpose**: Find the **shortest path** from a **source node** to **all other nodes** in a **weighted graph with non-negative edge weights**.

---

### üß† How it Works:

- Uses a **priority queue** (min-heap).
- Maintains a **distance array** (initially all ‚àû, source = 0).
- Always chooses the **closest unvisited node**.
- Updates distances to its neighbors (relaxation).
- Once a node is visited, its shortest path is final.

---

### üßÆ Pseudocode (simplified):

```cpp
function Dijkstra(Graph, source):
    dist[] = {‚àû, ‚àû, ..., ‚àû}
    dist[source] = 0
    minHeap = { (0, source) }

    while minHeap not empty:
        (d, u) = minHeap.pop()
        for each neighbor v of u:
            if dist[v] > dist[u] + weight(u, v):
                dist[v] = dist[u] + weight(u, v)
                minHeap.push((dist[v], v))
```

---

### Example:

Graph:
```
     (2)
 A ------- B
 |         |
(1)       (3)
 |         |
 C ------- D
     (1)
```

- Source = A  
- Dijkstra gives shortest paths:
  - A ‚Üí A: 0
  - A ‚Üí C: 1
  - A ‚Üí D: 2
  - A ‚Üí B: 2

---

## üå≤ Minimum Spanning Tree (MST)

**Goal**: Connect **all vertices** of a weighted undirected graph with **minimum total edge weight** and **no cycles**.

- MST has exactly **V-1 edges**
- **Multiple MSTs** may exist if weights are not unique.

---

## üîß Prim‚Äôs Algorithm

**Purpose**: Find the **MST** using a **greedy** approach.

---

### üß† How it Works:

- Start with any node.
- Grow the tree by **adding the smallest edge** connecting the tree to a new node.
- Use a **min-heap** to choose the smallest edge.

---

### üßÆ Pseudocode (simplified):

```cpp
function Prim(Graph, start):
    visited[] = false
    minHeap = { (0, start) }
    totalCost = 0

    while minHeap not empty:
        (cost, u) = minHeap.pop()
        if visited[u]: continue
        visited[u] = true
        totalCost += cost

        for each (v, weight) adjacent to u:
            if not visited[v]:
                minHeap.push((weight, v))
```

---

### Prim vs Dijkstra

| Feature             | Prim's                        | Dijkstra's                      |
|---------------------|-------------------------------|----------------------------------|
| Goal                | Minimum Spanning Tree (MST)   | Shortest paths from a source     |
| Graph Type          | Undirected, weighted          | Directed or undirected, weighted (no negative) |
| Strategy            | Greedy                        | Greedy with relaxation           |
| Starts With         | Any node                      | Source node                      |

---

### MST Example:

Graph:
```
   (2)
A------B
| \    |
1  3   4
|   \  |
C------D
    (5)
```

**Prim‚Äôs MST** might include:
- A‚ÄîC (1)
- A‚ÄîB (2)
- C‚ÄîD (3)

Total cost: **1 + 2 + 3 = 6**

---

Great! Let‚Äôs dive into **Kruskal‚Äôs Algorithm** ‚Äî another essential way to build a **Minimum Spanning Tree (MST)**. It‚Äôs also **greedy**, but works very differently from Prim‚Äôs.

---

## üåâ Kruskal‚Äôs Algorithm

### üéØ **Goal**: Build an MST by **adding the smallest edges** first, **avoiding cycles**.

---

### üß† How It Works:

1. Sort **all edges** by weight (ascending).
2. Use a **Disjoint Set (Union-Find)** structure to track connected components.
3. Start adding edges:
   - Add the **smallest edge** that **doesn‚Äôt create a cycle**.
   - Repeat until you‚Äôve added **V - 1 edges** (where V is number of vertices).

---

### üßÆ Pseudocode (simplified):

```cpp
function Kruskal(Graph):
    sort edges by weight
    create disjoint sets for all vertices
    MST = []

    for each edge (u, v) in sorted edges:
        if Find(u) ‚â† Find(v):  // no cycle
            Union(u, v)
            MST.add((u, v))

    return MST
```

‚úÖ Uses **Union-Find (Disjoint Set Union - DSU)**:
- `Find(x)`: finds the root of the set containing x.
- `Union(x, y)`: joins the sets containing x and y.

With path compression + union by rank: **O(E log V)** runtime.

---

### üìà Example:

Graph:

```
   (1)
A------C
|\     |
| \    |
4  3   2
|   \  |
B-----D
   (5)
```

**Edges sorted by weight**:
- A‚ÄìC (1)
- C‚ÄìD (2)
- A‚ÄìD (3)
- A‚ÄìB (4)
- B‚ÄìD (5)

Steps:
1. Add A‚ÄìC ‚Üí no cycle ‚úÖ
2. Add C‚ÄìD ‚Üí no cycle ‚úÖ
3. Add A‚ÄìD ‚Üí would create cycle ‚ùå
4. Add A‚ÄìB ‚Üí no cycle ‚úÖ

‚úÖ MST edges: A‚ÄìC, C‚ÄìD, A‚ÄìB  
**Total weight**: 1 + 2 + 4 = **7**

---

### ü•ä Prim vs Kruskal ‚Äì Quick Recap

| Feature        | Prim‚Äôs                       | Kruskal‚Äôs                     |
|----------------|-------------------------------|-------------------------------|
| Starts With    | Any node                      | Edges (sorted by weight)      |
| Strategy       | Grows connected component     | Connects components (sets)    |
| Uses           | Priority Queue (Min-Heap)     | Union-Find                    |
| Best For       | Dense graphs                  | Sparse graphs                 |

---

