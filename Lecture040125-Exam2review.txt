Lecture 4/1/2025 
- this one is from Slides 9 onward

- Heap will be on this tests
- Starting from Heap to Minimum Spanning Tree

### ğŸ”º What is a Heap?

A **heap** is a special type of **binary tree** that satisfies the following properties:

1. **Complete Binary Tree**: Every level is fully filled except possibly the last, which is filled from left to right.
2. **Heap Property**: The value of each node is **ordered with respect to its children**.

There are two main types:

### âœ… Min-Heap

- **Definition**: A binary heap where **each parent node is less than or equal to its children**.
- **Property**: The **minimum element is always at the root**.
- **Use Case**: Priority queues (where the smallest value gets removed first).

```
        2
      /   \
     4     5
    / \   / 
   9  10 8
```

---

### âœ… Max-Heap

- **Definition**: A binary heap where **each parent node is greater than or equal to its children**.
- **Property**: The **maximum element is always at the root**.
- **Use Case**: Priority queues where highest priority = largest number.

```
        10
       /  \
      9    5
     / \   /
    2   4 3
```

---

### ğŸ“¦ Array Representation of a Heap

We store the heap as an array where:

- **Root is at index 0** (or index 1 depending on convention).
- For a node at index `i`:
  - **Left child** = `2*i + 1`
  - **Right child** = `2*i + 2`
  - **Parent** = `(i - 1) / 2` (integer division)

**Example Min-Heap stored as array:**

```
Heap Tree:
        2
      /   \
     4     5
    / \   /
   9 10  8

Array: [2, 4, 5, 9, 10, 8]
```

---

### â¬ Heap Operations (Min-Heap)

- **Insert**:
  - Add at the end of array, then **bubble up** (swap with parent until heap property is restored).
- **Extract Min**:
  - Remove root (index 0), move last element to root, then **heapify down** (swap with smaller child until heap property restored).
- **Build Heap**:
  - From unsorted array in O(n) time using `heapify()` from bottom-up.

#### Adding & Delting Nodes
- New nodes are always inserted at the bottom level (Left to right)
- Nodes are removed from the bottom level (right to left)

- Max-Heap-Insert Algorithm
Alg(MAX-HEAP-INSERT(A, key, n)
1. heap-size[A] <- n + 1
2. A[n + 1] <- -infinity
3. HEAP-INCREASE-KEY(A, n + 1, key)
-- running time is O(log n)



---

### ğŸŒ³ What is a Binary Search Tree (BST)?

A **Binary Search Tree** is a **binary tree** with the following properties:

1. **Left Subtree < Root**: All nodes in the left subtree have **values less than the root**.
2. **Right Subtree > Root**: All nodes in the right subtree have **values greater than the root**.
3. **No duplicates** (in standard BSTs).
4. This rule applies **recursively** to every subtree.

---

### ğŸ“ˆ Example of a BST

```
        8
       / \
      3   10
     / \    \
    1   6    14
       / \   /
      4   7 13
```

---

### ğŸ§  Key BST Operations

| Operation     | Description                                      | Time (avg) | Time (worst) |
|---------------|--------------------------------------------------|------------|---------------|
| **Insert**     | Add a value by traversing left/right.            | O(log n)   | O(n)         |
| **Search**     | Look for a value via comparisons.                | O(log n)   | O(n)         |
| **Delete**     | Remove a node, handle 3 cases (see below).       | O(log n)   | O(n)         |
| **Traversal**  | In-order (sorted), Pre-order, Post-order, etc.   | O(n)       | O(n)         |

---

### ğŸ—‘ï¸ Deletion in BST (3 Cases)

1. **Node is a leaf**: Just delete it.
2. **Node has one child**: Replace it with its child.
3. **Node has two children**:
   - Find **in-order successor** (smallest in right subtree) or **in-order predecessor** (largest in left subtree).
   - Replace nodeâ€™s value with that.
   - Delete successor/predecessor node.

---

### ğŸ” BST Traversals

- **In-order (LNR)**: Left â†’ Node â†’ Right â†’ gives **sorted order**.
- **Pre-order (NLR)**: Node â†’ Left â†’ Right â†’ useful for copying the tree.
- **Post-order (LRN)**: Left â†’ Right â†’ Node â†’ used to delete the tree.

---

### â—Limitations

- Not balanced â†’ can degrade to **linked list** (e.g., inserting sorted elements).
- Worst case time: O(n)
  
âœ… Thatâ€™s why we have **balanced BSTs** like **AVL Tree** or **Red-Black Tree** (which we'll cover next if you want).

Awesome â€” letâ€™s talk about **balancing a Binary Search Tree (BST)**, which is crucial for keeping operations fast. Here's your quick, clear breakdown:

---

### âš–ï¸ Why Balance a BST?

- **Unbalanced BST**: If you insert elements in sorted order (e.g., 1, 2, 3, 4â€¦), the BST becomes a **linked list**:
  ```
  1
   \
    2
     \
      3
       \
        4
  ```
  - Operations like search, insert, and delete degrade from **O(log n)** to **O(n)**.

- **Balanced BST**: Keeps the treeâ€™s height as small as possible, ideally **O(log n)**. This ensures operations remain efficient.

---

### ğŸ› ï¸ Types of Balanced Trees

#### 1. **AVL Tree**
- **Self-balancing BST**: After each insert/delete, it checks balance.
- **Balance factor** = height(left subtree) â€“ height(right subtree)
  - Must be -1, 0, or +1.
- If not, perform **rotations** (single or double) to rebalance.

âœ… Rotations:
- **Right Rotation (RR)**: Used when left-left heavy.
- **Left Rotation (LL)**: Used when right-right heavy.
- **Left-Right (LR)** and **Right-Left (RL)**: Two-step rotations.

#### 2. **Red-Black Tree**
- A BST where each node is **red or black**, with rules to maintain balance.
- Ensures the **longest path is no more than twice the shortest**.
- Used in many real-world systems (e.g., Java TreeMap, Linux scheduler).

Red-Black Rules:
1. Every node is red or black.
2. Root is always black.
3. No two red nodes in a row.
4. Every path from a node to null must have the same number of black nodes.

ğŸ”„ Insertion/Deletion may trigger **color flips** or **rotations**.

#### 3. **Splay Tree**
- Recently accessed elements are moved to the root using rotations.
- Helps with **temporal locality** (if you access the same data repeatedly).

---

### â³ Summary of Time Complexities

| Tree Type     | Insert | Search | Delete |
|---------------|--------|--------|--------|
| BST (avg)     | O(log n) | O(log n) | O(log n) |
| BST (worst)   | O(n)     | O(n)     | O(n)     |
| AVL Tree      | O(log n) | O(log n) | O(log n) |
| Red-Black Tree| O(log n) | O(log n) | O(log n) |

---
Examples of Rotation: 

Weâ€™ll do:

1. **Right Rotation (LL case)**  
2. **Left Rotation (RR case)**  
3. **Left-Right Rotation (LR case)**  
4. **Right-Left Rotation (RL case)**

---

### 1ï¸âƒ£ Right Rotation (LL Case)

**Trigger**: Inserting into the **left subtree of the left child**.

#### Insert: 30 â†’ 20 â†’ 10

```
Before rotation:
        30
       /
     20
    /
  10

Balance factor at 30 = 2 (left-heavy)
```

**Right Rotation around 30:**

```
After rotation:
      20
     /  \
   10    30
```

---

### 2ï¸âƒ£ Left Rotation (RR Case)

**Trigger**: Inserting into the **right subtree of the right child**.

#### Insert: 10 â†’ 20 â†’ 30

```
Before rotation:
    10
      \
       20
         \
          30

Balance factor at 10 = -2 (right-heavy)
```

**Left Rotation around 10:**

```
After rotation:
       20
      /  \
    10    30
```

---

### 3ï¸âƒ£ Left-Right Rotation (LR Case)

**Trigger**: Inserting into the **right subtree of the left child**.

#### Insert: 30 â†’ 10 â†’ 20

```
Before rotation:
        30
       /
     10
       \
        20

Balance factor at 30 = 2 (left-heavy), but left child is right-heavy.
```

**Step 1: Left rotation at 10**

```
        30
       /
     20
     /
   10
```

**Step 2: Right rotation at 30**

```
      20
     /  \
   10    30
```

---

### 4ï¸âƒ£ Right-Left Rotation (RL Case)

**Trigger**: Inserting into the **left subtree of the right child**.

#### Insert: 10 â†’ 30 â†’ 20

```
Before rotation:
    10
      \
       30
      /
    20

Balance factor at 10 = -2 (right-heavy), but right child is left-heavy.
```

**Step 1: Right rotation at 30**

```
    10
      \
       20
         \
          30
```

**Step 2: Left rotation at 10**

```
      20
     /  \
   10    30
```

---


---

### ğŸ”‘ What is Hashing?

**Hashing** is the process of converting a key into an index using a **hash function**, so data can be inserted, searched, or deleted in near **constant time** (O(1) average case).

---

### ğŸ§  Hash Table

- **Data structure** that stores key-value pairs.
- Uses a **hash function** to compute an index in an array (the table).
- Ideal for quick lookups like dictionaries, symbol tables, caches.

---

### âš™ï¸ Hash Function

A **hash function** `h(key)` maps a key to an index in the table.

Requirements:
- Must be **fast**
- Should **distribute keys uniformly**
- Must be **deterministic** (same key â†’ same hash)

Example (simple):
```cpp
int hash(int key) {
    return key % table_size;
}
```

---

### âŒ Collisions

When two keys hash to the **same index**, it's called a **collision**.

---

### ğŸ› ï¸ Collision Resolution Techniques

#### 1. **Chaining (Separate Chaining)**
- Each slot holds a **linked list** of entries.
- When collision occurs, just add to the list at that index.

```
Table[2]: â†’ (12) â†’ (22)
```

#### 2. **Open Addressing**
- All data is stored directly in the table (no extra structure).
- When a collision happens, probe for the next empty slot.

**Probing methods:**
- **Linear Probing**: `h(k), h(k)+1, h(k)+2, ...`
- **Quadratic Probing**: `h(k), h(k)+1Â², h(k)+2Â², ...`
- **Double Hashing**: Use second hash function for jump size.

---

### ğŸ“ˆ Load Factor (Î±)

- **Î± = n / table_size**
- Ratio of stored elements to table size.
- High Î± â‡’ more collisions.
- Keep **Î± < 0.7** for good performance (esp. in open addressing).

---

### ğŸ” Rehashing

- When load factor gets too high, create a new larger table and re-insert all keys.
- Often double the size (next prime is ideal).

---

### âœ… Hash Table Time Complexity

| Operation | Avg Time | Worst Time |
|-----------|----------|------------|
| Insert    | O(1)     | O(n)       |
| Search    | O(1)     | O(n)       |
| Delete    | O(1)     | O(n)       |

(Avg case assumes good hash function and low collisions.)

---

### ğŸ§ª Example:

```cpp
table_size = 7
keys = [10, 20, 15, 7]
hash(key) = key % 7

Indexes:
10 % 7 = 3 â†’ table[3]
20 % 7 = 6 â†’ table[6]
15 % 7 = 1 â†’ table[1]
7  % 7 = 0 â†’ table[0]
```

---

Absolutely! Letâ€™s look at **examples** of the three main **open addressing** methods in hash tables:

We'll use:
- **Hash function**: `h(key) = key % 7`
- **Table size**: `7`
- We'll insert: `10, 20, 15, 7, 32`

---

### ğŸ“¦ Initial Setup:

```plaintext
Empty table (size = 7):
[0] _
[1] _
[2] _
[3] _
[4] _
[5] _
[6] _
```

---

### ğŸ” 1. **Linear Probing**

If collision occurs, probe **next slot (i + 1)** until empty.

**Insert 10** â†’ 10 % 7 = 3  
â†’ table[3] is empty â†’ insert at index 3

**Insert 20** â†’ 20 % 7 = 6  
â†’ table[6] is empty â†’ insert at index 6

**Insert 15** â†’ 15 % 7 = 1  
â†’ table[1] is empty â†’ insert at index 1

**Insert 7** â†’ 7 % 7 = 0  
â†’ table[0] is empty â†’ insert at index 0

**Insert 32** â†’ 32 % 7 = 4  
â†’ table[4] is empty â†’ insert at index 4

âœ… Final Table:
```plaintext
[0]  7
[1] 15
[2] _
[3] 10
[4] 32
[5] _
[6] 20
```

Now let's see what happens **with a collision**:

**Insert 17** â†’ 17 % 7 = 3 â†’ table[3] is full  
Linear probing checks:
- table[4] â†’ full  
- table[5] â†’ empty â†’ insert at 5

âœ… Table now:
```plaintext
[0]  7
[1] 15
[2] _
[3] 10
[4] 32
[5] 17
[6] 20
```

---

### ğŸ” 2. **Quadratic Probing**

Probes in steps of `iÂ²`:  
index = `(h(key) + iÂ²) % table_size`, for i = 0, 1, 2...

**Insert 10** â†’ 10 % 7 = 3 â†’ table[3] = empty â†’ insert

**Insert 17** â†’ 17 % 7 = 3 â†’ collision  
Try:
- i=1 â†’ (3 + 1Â²) % 7 = 4 â†’ empty â†’ insert at index 4

**Insert 24** â†’ 24 % 7 = 3 â†’ collision  
Try:
- i=1 â†’ (3 + 1Â²) % 7 = 4 â†’ full  
- i=2 â†’ (3 + 4) % 7 = 0 â†’ insert at index 0

âœ… Table:
```plaintext
[0] 24
[1] _
[2] _
[3] 10
[4] 17
[5] _
[6] _
```

---

### ğŸ” 3. **Double Hashing**

Use two hash functions:
- `h1(key) = key % 7`
- `h2(key) = 5 - (key % 5)` (secondary hash must not be 0!)

Probe: `(h1 + i * h2) % table_size`

**Insert 10**:  
- h1 = 10 % 7 = 3  
- h2 = 5 - (10 % 5) = 5 - 0 = 5  
â†’ table[3] = empty â†’ insert

**Insert 17**:  
- h1 = 3, h2 = 3  
- index = (3 + 0Ã—3) % 7 = 3 â†’ collision  
- i=1 â†’ (3 + 1Ã—3) % 7 = 6 â†’ insert at 6

**Insert 24**:  
- h1 = 3, h2 = 1  
- i=0 â†’ 3  
- i=1 â†’ 4 â†’ insert at 4

âœ… Table:
```plaintext
[0] _
[1] _
[2] _
[3] 10
[4] 24
[5] _
[6] 17
```

---

### ğŸ§  Summary

| Method            | Probe Sequence (on collision)           |
|------------------|------------------------------------------|
| Linear Probing   | h, h+1, h+2, ...                         |
| Quadratic Probing| h, h+1Â², h+2Â², ...                       |
| Double Hashing   | h1, h1 + h2, h1 + 2*h2, ...              |

Each method has tradeoffs:
- **Linear probing**: Simple, but causes **clustering**.
- **Quadratic probing**: Reduces clustering, but may fail to find empty slot if table is too full.
- **Double hashing**: Best distribution but needs good second hash.

---

Awesome! Letâ€™s tackle **Graphs** next â€” an essential part of Data Structures and Algorithms. Weâ€™ll go step by step: definition â BFS â DFS â comparisons.

---

### ğŸŒ What is a Graph?

A **graph** is a collection of:

- **Vertices (nodes)**: Represent entities.
- **Edges (connections)**: Represent relationships between vertices.

Graphs can be:

1. **Directed** or **Undirected**  
   - Directed: edges have a direction (A â B)  
   - Undirected: edges go both ways (A â€” B)

2. **Weighted** or **Unweighted**  
   - Weighted: edges have costs/weights (like distance, time, etc.)  
   - Unweighted: all edges treated equally

3. **Cyclic** or **Acyclic**  
   - Cyclic: contains cycles  
   - Acyclic: no cycles (e.g., DAG â€“ Directed Acyclic Graph)

4. **Connected** or **Disconnected**  
   - Connected: there's a path between every pair of vertices  
   - Disconnected: not all nodes are reachable from each other

---

### ğŸ“Š Graph Representations

1. **Adjacency Matrix**  
   - 2D array where `matrix[i][j] = 1` if thereâ€™s an edge from `i` to `j`
   - Pros: Fast lookup  
   - Cons: Space = O(VÂ²)

2. **Adjacency List**  
   - Array of lists: each index `i` stores all neighbors of vertex `i`  
   - Pros: Space = O(V + E), better for sparse graphs

---

### ğŸ” Breadth-First Search (BFS)

- **Level-order traversal** of a graph.
- Uses a **queue (FIFO)**.
- Good for **finding shortest path (unweighted)**, or all nodes reachable from a start node.

#### Pseudocode:
```cpp
BFS(Graph, start):
  create empty queue Q
  mark start as visited
  enqueue start into Q

  while Q is not empty:
    node = Q.dequeue()
    for each neighbor of node:
      if not visited:
        mark visited
        enqueue neighbor
```

#### Example:

Graph:
```
A -- B -- C
|         |
D ------- E
```

**BFS from A** â†’ `A, B, D, C, E`

---

### ğŸ” Depth-First Search (DFS)

- **Explore as deep as possible** before backtracking.
- Uses **stack** (explicit or via recursion).
- Good for path-finding, cycle detection, topological sort, etc.

#### Pseudocode (recursive):
```cpp
DFS(Graph, node):
  mark node as visited
  for each neighbor:
    if not visited:
      DFS(neighbor)
```

**DFS from A** â†’ `A, B, C, E, D` *(depends on adjacency order)*

---

### ğŸ” BFS vs. DFS

| Feature            | BFS                         | DFS                         |
|--------------------|-----------------------------|-----------------------------|
| Uses               | Queue                       | Stack / Recursion           |
| Traversal Order    | Level by level              | Deep before wide            |
| Space              | O(V)                        | O(V)                        |
| Shortest Path      | âœ… Yes (unweighted graphs)   | âŒ No                        |
| Cycle Detection    | âœ… (with parent tracking)    | âœ…                          |
| Complete?          | Yes                         | Yes                         |

---

### ğŸ“š Common Applications

- **BFS**: Shortest path (unweighted), social networks, web crawlers  
- **DFS**: Topological sort, detecting cycles, solving puzzles (e.g., maze), connected components

---

Awesome â€” youâ€™re covering some of the most powerful graph algorithms! Letâ€™s go over:

1. âœ… **Dijkstraâ€™s Algorithm** (for shortest paths)  
2. âœ… **Minimum Spanning Tree (MST)**  
3. âœ… **Primâ€™s Algorithm** (to find MST)

---

## ğŸš€ Dijkstraâ€™s Algorithm

**Purpose**: Find the **shortest path** from a **source node** to **all other nodes** in a **weighted graph with non-negative edge weights**.

---

### ğŸ§  How it Works:

- Uses a **priority queue** (min-heap).
- Maintains a **distance array** (initially all âˆ, source = 0).
- Always chooses the **closest unvisited node**.
- Updates distances to its neighbors (relaxation).
- Once a node is visited, its shortest path is final.

---

### ğŸ§® Pseudocode (simplified):

```cpp
function Dijkstra(Graph, source):
    dist[] = {âˆ, âˆ, ..., âˆ}
    dist[source] = 0
    minHeap = { (0, source) }

    while minHeap not empty:
        (d, u) = minHeap.pop()
        for each neighbor v of u:
            if dist[v] > dist[u] + weight(u, v):
                dist[v] = dist[u] + weight(u, v)
                minHeap.push((dist[v], v))
```

---

### Example:

Graph:
```
     (2)
 A ------- B
 |         |
(1)       (3)
 |         |
 C ------- D
     (1)
```

- Source = A  
- Dijkstra gives shortest paths:
  - A â†’ A: 0
  - A â†’ C: 1
  - A â†’ D: 2
  - A â†’ B: 2

---

## ğŸŒ² Minimum Spanning Tree (MST)

**Goal**: Connect **all vertices** of a weighted undirected graph with **minimum total edge weight** and **no cycles**.

- MST has exactly **V-1 edges**
- **Multiple MSTs** may exist if weights are not unique.

---

## ğŸ”§ Primâ€™s Algorithm

**Purpose**: Find the **MST** using a **greedy** approach.

---

### ğŸ§  How it Works:

- Start with any node.
- Grow the tree by **adding the smallest edge** connecting the tree to a new node.
- Use a **min-heap** to choose the smallest edge.

---

### ğŸ§® Pseudocode (simplified):

```cpp
function Prim(Graph, start):
    visited[] = false
    minHeap = { (0, start) }
    totalCost = 0

    while minHeap not empty:
        (cost, u) = minHeap.pop()
        if visited[u]: continue
        visited[u] = true
        totalCost += cost

        for each (v, weight) adjacent to u:
            if not visited[v]:
                minHeap.push((weight, v))
```

---

### Prim vs Dijkstra

| Feature             | Prim's                        | Dijkstra's                      |
|---------------------|-------------------------------|----------------------------------|
| Goal                | Minimum Spanning Tree (MST)   | Shortest paths from a source     |
| Graph Type          | Undirected, weighted          | Directed or undirected, weighted (no negative) |
| Strategy            | Greedy                        | Greedy with relaxation           |
| Starts With         | Any node                      | Source node                      |

---

### MST Example:

Graph:
```
   (2)
A------B
| \    |
1  3   4
|   \  |
C------D
    (5)
```

**Primâ€™s MST** might include:
- Aâ€”C (1)
- Aâ€”B (2)
- Câ€”D (3)

Total cost: **1 + 2 + 3 = 6**

---

Great! Letâ€™s dive into **Kruskalâ€™s Algorithm** â€” another essential way to build a **Minimum Spanning Tree (MST)**. Itâ€™s also **greedy**, but works very differently from Primâ€™s.

---

## ğŸŒ‰ Kruskalâ€™s Algorithm

### ğŸ¯ **Goal**: Build an MST by **adding the smallest edges** first, **avoiding cycles**.

---

### ğŸ§  How It Works:

1. Sort **all edges** by weight (ascending).
2. Use a **Disjoint Set (Union-Find)** structure to track connected components.
3. Start adding edges:
   - Add the **smallest edge** that **doesnâ€™t create a cycle**.
   - Repeat until youâ€™ve added **V - 1 edges** (where V is number of vertices).

---

### ğŸ§® Pseudocode (simplified):

```cpp
function Kruskal(Graph):
    sort edges by weight
    create disjoint sets for all vertices
    MST = []

    for each edge (u, v) in sorted edges:
        if Find(u) â‰  Find(v):  // no cycle
            Union(u, v)
            MST.add((u, v))

    return MST
```

âœ… Uses **Union-Find (Disjoint Set Union - DSU)**:
- `Find(x)`: finds the root of the set containing x.
- `Union(x, y)`: joins the sets containing x and y.

With path compression + union by rank: **O(E log V)** runtime.

---

### ğŸ“ˆ Example:

Graph:

```
   (1)
A------C
|\     |
| \    |
4  3   2
|   \  |
B-----D
   (5)
```

**Edges sorted by weight**:
- Aâ€“C (1)
- Câ€“D (2)
- Aâ€“D (3)
- Aâ€“B (4)
- Bâ€“D (5)

Steps:
1. Add Aâ€“C â†’ no cycle âœ…
2. Add Câ€“D â†’ no cycle âœ…
3. Add Aâ€“D â†’ would create cycle âŒ
4. Add Aâ€“B â†’ no cycle âœ…

âœ… MST edges: Aâ€“C, Câ€“D, Aâ€“B  
**Total weight**: 1 + 2 + 4 = **7**

---

### ğŸ¥Š Prim vs Kruskal â€“ Quick Recap

| Feature        | Primâ€™s                       | Kruskalâ€™s                     |
|----------------|-------------------------------|-------------------------------|
| Starts With    | Any node                      | Edges (sorted by weight)      |
| Strategy       | Grows connected component     | Connects components (sets)    |
| Uses           | Priority Queue (Min-Heap)     | Union-Find                    |
| Best For       | Dense graphs                  | Sparse graphs                 |

---

