Lecture 4/1/2025 
- this one is from Slides 9 onward

- Heap will be on this tests
- Starting from Heap to Minimum Spanning Tree

### üî∫ What is a Heap?

A **heap** is a special type of **binary tree** that satisfies the following properties:

1. **Complete Binary Tree**: Every level is fully filled except possibly the last, which is filled from left to right.
2. **Heap Property**: The value of each node is **ordered with respect to its children**.

There are two main types:

### ‚úÖ Min-Heap

- **Definition**: A binary heap where **each parent node is less than or equal to its children**.
- **Property**: The **minimum element is always at the root**.
- **Use Case**: Priority queues (where the smallest value gets removed first).

```
        2
      /   \
     4     5
    / \   / 
   9  10 8
```

---

### ‚úÖ Max-Heap

- **Definition**: A binary heap where **each parent node is greater than or equal to its children**.
- **Property**: The **maximum element is always at the root**.
- **Use Case**: Priority queues where highest priority = largest number.

```
        10
       /  \
      9    5
     / \   /
    2   4 3
```

---

### üì¶ Array Representation of a Heap

We store the heap as an array where:

- **Root is at index 0** (or index 1 depending on convention).
- For a node at index `i`:
  - **Left child** = `2*i + 1`
  - **Right child** = `2*i + 2`
  - **Parent** = `(i - 1) / 2` (integer division)

**Example Min-Heap stored as array:**

```
Heap Tree:
        2
      /   \
     4     5
    / \   /
   9 10  8

Array: [2, 4, 5, 9, 10, 8]
```

---

### ‚è¨ Heap Operations (Min-Heap)

- **Insert**:
  - Add at the end of array, then **bubble up** (swap with parent until heap property is restored).
- **Extract Min**:
  - Remove root (index 0), move last element to root, then **heapify down** (swap with smaller child until heap property restored).
- **Build Heap**:
  - From unsorted array in O(n) time using `heapify()` from bottom-up.

#### Adding & Delting Nodes
- New nodes are always inserted at the bottom level (Left to right)
- Nodes are removed from the bottom level (right to left)

- Max-Heap-Insert Algorithm
Alg(MAX-HEAP-INSERT(A, key, n)
1. heap-size[A] <- n + 1
2. A[n + 1] <- -infinity
3. HEAP-INCREASE-KEY(A, n + 1, key)
-- running time is O(log n)



---

### üå≥ What is a Binary Search Tree (BST)?

A **Binary Search Tree** is a **binary tree** with the following properties:

1. **Left Subtree < Root**: All nodes in the left subtree have **values less than the root**.
2. **Right Subtree > Root**: All nodes in the right subtree have **values greater than the root**.
3. **No duplicates** (in standard BSTs).
4. This rule applies **recursively** to every subtree.

---

### üìà Example of a BST

```
        8
       / \
      3   10
     / \    \
    1   6    14
       / \   /
      4   7 13
```

---

### üß† Key BST Operations

| Operation     | Description                                      | Time (avg) | Time (worst) |
|---------------|--------------------------------------------------|------------|---------------|
| **Insert**     | Add a value by traversing left/right.            | O(log n)   | O(n)         |
| **Search**     | Look for a value via comparisons.                | O(log n)   | O(n)         |
| **Delete**     | Remove a node, handle 3 cases (see below).       | O(log n)   | O(n)         |
| **Traversal**  | In-order (sorted), Pre-order, Post-order, etc.   | O(n)       | O(n)         |

---

### üóëÔ∏è Deletion in BST (3 Cases)

1. **Node is a leaf**: Just delete it.
2. **Node has one child**: Replace it with its child.
3. **Node has two children**:
   - Find **in-order successor** (smallest in right subtree) or **in-order predecessor** (largest in left subtree).
   - Replace node‚Äôs value with that.
   - Delete successor/predecessor node.

---

### üîÅ BST Traversals

- **In-order (LNR)**: Left ‚Üí Node ‚Üí Right ‚Üí gives **sorted order**.
- **Pre-order (NLR)**: Node ‚Üí Left ‚Üí Right ‚Üí useful for copying the tree.
- **Post-order (LRN)**: Left ‚Üí Right ‚Üí Node ‚Üí used to delete the tree.

---

### ‚ùóLimitations

- Not balanced ‚Üí can degrade to **linked list** (e.g., inserting sorted elements).
- Worst case time: O(n)
  
‚úÖ That‚Äôs why we have **balanced BSTs** like **AVL Tree** or **Red-Black Tree** (which we'll cover next if you want).

Awesome ‚Äî let‚Äôs talk about **balancing a Binary Search Tree (BST)**, which is crucial for keeping operations fast. Here's your quick, clear breakdown:

---

### ‚öñÔ∏è Why Balance a BST?

- **Unbalanced BST**: If you insert elements in sorted order (e.g., 1, 2, 3, 4‚Ä¶), the BST becomes a **linked list**:
  ```
  1
   \
    2
     \
      3
       \
        4
  ```
  - Operations like search, insert, and delete degrade from **O(log n)** to **O(n)**.

- **Balanced BST**: Keeps the tree‚Äôs height as small as possible, ideally **O(log n)**. This ensures operations remain efficient.

---

### üõ†Ô∏è Types of Balanced Trees

#### 1. **AVL Tree**
- **Self-balancing BST**: After each insert/delete, it checks balance.
- **Balance factor** = height(left subtree) ‚Äì height(right subtree)
  - Must be -1, 0, or +1.
- If not, perform **rotations** (single or double) to rebalance.

‚úÖ Rotations:
- **Right Rotation (RR)**: Used when left-left heavy.
- **Left Rotation (LL)**: Used when right-right heavy.
- **Left-Right (LR)** and **Right-Left (RL)**: Two-step rotations.

#### 2. **Red-Black Tree**
- A BST where each node is **red or black**, with rules to maintain balance.
- Ensures the **longest path is no more than twice the shortest**.
- Used in many real-world systems (e.g., Java TreeMap, Linux scheduler).

Red-Black Rules:
1. Every node is red or black.
2. Root is always black.
3. No two red nodes in a row.
4. Every path from a node to null must have the same number of black nodes.

üîÑ Insertion/Deletion may trigger **color flips** or **rotations**.

#### 3. **Splay Tree**
- Recently accessed elements are moved to the root using rotations.
- Helps with **temporal locality** (if you access the same data repeatedly).

---

### ‚è≥ Summary of Time Complexities

| Tree Type     | Insert | Search | Delete |
|---------------|--------|--------|--------|
| BST (avg)     | O(log n) | O(log n) | O(log n) |
| BST (worst)   | O(n)     | O(n)     | O(n)     |
| AVL Tree      | O(log n) | O(log n) | O(log n) |
| Red-Black Tree| O(log n) | O(log n) | O(log n) |

---
Examples of Rotation: 

We‚Äôll do:

1. **Right Rotation (LL case)**  
2. **Left Rotation (RR case)**  
3. **Left-Right Rotation (LR case)**  
4. **Right-Left Rotation (RL case)**

---

### 1Ô∏è‚É£ Right Rotation (LL Case)

**Trigger**: Inserting into the **left subtree of the left child**.

#### Insert: 30 ‚Üí 20 ‚Üí 10

```
Before rotation:
        30
       /
     20
    /
  10

Balance factor at 30 = 2 (left-heavy)
```

**Right Rotation around 30:**

```
After rotation:
      20
     /  \
   10    30
```

---

### 2Ô∏è‚É£ Left Rotation (RR Case)

**Trigger**: Inserting into the **right subtree of the right child**.

#### Insert: 10 ‚Üí 20 ‚Üí 30

```
Before rotation:
    10
      \
       20
         \
          30

Balance factor at 10 = -2 (right-heavy)
```

**Left Rotation around 10:**

```
After rotation:
       20
      /  \
    10    30
```

---

### 3Ô∏è‚É£ Left-Right Rotation (LR Case)

**Trigger**: Inserting into the **right subtree of the left child**.

#### Insert: 30 ‚Üí 10 ‚Üí 20

```
Before rotation:
        30
       /
     10
       \
        20

Balance factor at 30 = 2 (left-heavy), but left child is right-heavy.
```

**Step 1: Left rotation at 10**

```
        30
       /
     20
     /
   10
```

**Step 2: Right rotation at 30**

```
      20
     /  \
   10    30
```

---

### 4Ô∏è‚É£ Right-Left Rotation (RL Case)

**Trigger**: Inserting into the **left subtree of the right child**.

#### Insert: 10 ‚Üí 30 ‚Üí 20

```
Before rotation:
    10
      \
       30
      /
    20

Balance factor at 10 = -2 (right-heavy), but right child is left-heavy.
```

**Step 1: Right rotation at 30**

```
    10
      \
       20
         \
          30
```

**Step 2: Left rotation at 10**

```
      20
     /  \
   10    30
```

---


---

### üîë What is Hashing?

**Hashing** is the process of converting a key into an index using a **hash function**, so data can be inserted, searched, or deleted in near **constant time** (O(1) average case).

---

### üß† Hash Table

- **Data structure** that stores key-value pairs.
- Uses a **hash function** to compute an index in an array (the table).
- Ideal for quick lookups like dictionaries, symbol tables, caches.

---

### ‚öôÔ∏è Hash Function

A **hash function** `h(key)` maps a key to an index in the table.

Requirements:
- Must be **fast**
- Should **distribute keys uniformly**
- Must be **deterministic** (same key ‚Üí same hash)

Example (simple):
```cpp
int hash(int key) {
    return key % table_size;
}
```

---

### ‚ùå Collisions

When two keys hash to the **same index**, it's called a **collision**.

---

### üõ†Ô∏è Collision Resolution Techniques

#### 1. **Chaining (Separate Chaining)**
- Each slot holds a **linked list** of entries.
- When collision occurs, just add to the list at that index.

```
Table[2]: ‚Üí (12) ‚Üí (22)
```

#### 2. **Open Addressing**
- All data is stored directly in the table (no extra structure).
- When a collision happens, probe for the next empty slot.

**Probing methods:**
- **Linear Probing**: `h(k), h(k)+1, h(k)+2, ...`
- **Quadratic Probing**: `h(k), h(k)+1¬≤, h(k)+2¬≤, ...`
- **Double Hashing**: Use second hash function for jump size.

---

### üìà Load Factor (Œ±)

- **Œ± = n / table_size**
- Ratio of stored elements to table size.
- High Œ± ‚áí more collisions.
- Keep **Œ± < 0.7** for good performance (esp. in open addressing).

---

### üîÅ Rehashing

- When load factor gets too high, create a new larger table and re-insert all keys.
- Often double the size (next prime is ideal).

---

### ‚úÖ Hash Table Time Complexity

| Operation | Avg Time | Worst Time |
|-----------|----------|------------|
| Insert    | O(1)     | O(n)       |
| Search    | O(1)     | O(n)       |
| Delete    | O(1)     | O(n)       |

(Avg case assumes good hash function and low collisions.)

---

### üß™ Example:

```cpp
table_size = 7
keys = [10, 20, 15, 7]
hash(key) = key % 7

Indexes:
10 % 7 = 3 ‚Üí table[3]
20 % 7 = 6 ‚Üí table[6]
15 % 7 = 1 ‚Üí table[1]
7  % 7 = 0 ‚Üí table[0]
```

---

Absolutely! Let‚Äôs look at **examples** of the three main **open addressing** methods in hash tables:

We'll use:
- **Hash function**: `h(key) = key % 7`
- **Table size**: `7`
- We'll insert: `10, 20, 15, 7, 32`

---

### üì¶ Initial Setup:

```plaintext
Empty table (size = 7):
[0] _
[1] _
[2] _
[3] _
[4] _
[5] _
[6] _
```

---

### üîÅ 1. **Linear Probing**

If collision occurs, probe **next slot (i + 1)** until empty.

**Insert 10** ‚Üí 10 % 7 = 3  
‚Üí table[3] is empty ‚Üí insert at index 3

**Insert 20** ‚Üí 20 % 7 = 6  
‚Üí table[6] is empty ‚Üí insert at index 6

**Insert 15** ‚Üí 15 % 7 = 1  
‚Üí table[1] is empty ‚Üí insert at index 1

**Insert 7** ‚Üí 7 % 7 = 0  
‚Üí table[0] is empty ‚Üí insert at index 0

**Insert 32** ‚Üí 32 % 7 = 4  
‚Üí table[4] is empty ‚Üí insert at index 4

‚úÖ Final Table:
```plaintext
[0]  7
[1] 15
[2] _
[3] 10
[4] 32
[5] _
[6] 20
```

Now let's see what happens **with a collision**:

**Insert 17** ‚Üí 17 % 7 = 3 ‚Üí table[3] is full  
Linear probing checks:
- table[4] ‚Üí full  
- table[5] ‚Üí empty ‚Üí insert at 5

‚úÖ Table now:
```plaintext
[0]  7
[1] 15
[2] _
[3] 10
[4] 32
[5] 17
[6] 20
```

---

### üîÅ 2. **Quadratic Probing**

Probes in steps of `i¬≤`:  
index = `(h(key) + i¬≤) % table_size`, for i = 0, 1, 2...

**Insert 10** ‚Üí 10 % 7 = 3 ‚Üí table[3] = empty ‚Üí insert

**Insert 17** ‚Üí 17 % 7 = 3 ‚Üí collision  
Try:
- i=1 ‚Üí (3 + 1¬≤) % 7 = 4 ‚Üí empty ‚Üí insert at index 4

**Insert 24** ‚Üí 24 % 7 = 3 ‚Üí collision  
Try:
- i=1 ‚Üí (3 + 1¬≤) % 7 = 4 ‚Üí full  
- i=2 ‚Üí (3 + 4) % 7 = 0 ‚Üí insert at index 0

‚úÖ Table:
```plaintext
[0] 24
[1] _
[2] _
[3] 10
[4] 17
[5] _
[6] _
```

---

### üîÅ 3. **Double Hashing**

Use two hash functions:
- `h1(key) = key % 7`
- `h2(key) = 5 - (key % 5)` (secondary hash must not be 0!)

Probe: `(h1 + i * h2) % table_size`

**Insert 10**:  
- h1 = 10 % 7 = 3  
- h2 = 5 - (10 % 5) = 5 - 0 = 5  
‚Üí table[3] = empty ‚Üí insert

**Insert 17**:  
- h1 = 3, h2 = 3  
- index = (3 + 0√ó3) % 7 = 3 ‚Üí collision  
- i=1 ‚Üí (3 + 1√ó3) % 7 = 6 ‚Üí insert at 6

**Insert 24**:  
- h1 = 3, h2 = 1  
- i=0 ‚Üí 3  
- i=1 ‚Üí 4 ‚Üí insert at 4

‚úÖ Table:
```plaintext
[0] _
[1] _
[2] _
[3] 10
[4] 24
[5] _
[6] 17
```

---

### üß† Summary

| Method            | Probe Sequence (on collision)           |
|------------------|------------------------------------------|
| Linear Probing   | h, h+1, h+2, ...                         |
| Quadratic Probing| h, h+1¬≤, h+2¬≤, ...                       |
| Double Hashing   | h1, h1 + h2, h1 + 2*h2, ...              |

Each method has tradeoffs:
- **Linear probing**: Simple, but causes **clustering**.
- **Quadratic probing**: Reduces clustering, but may fail to find empty slot if table is too full.
- **Double hashing**: Best distribution but needs good second hash.

---

Awesome! Let‚Äôs tackle **Graphs** next ‚Äî an essential part of Data Structures and Algorithms. We‚Äôll go step by step: definition ‚ûù BFS ‚ûù DFS ‚ûù comparisons.

---

### üåê What is a Graph?

A **graph** is a collection of:

- **Vertices (nodes)**: Represent entities.
- **Edges (connections)**: Represent relationships between vertices.

Graphs can be:

1. **Directed** or **Undirected**  
   - Directed: edges have a direction (A ‚ûù B)  
   - Undirected: edges go both ways (A ‚Äî B)

2. **Weighted** or **Unweighted**  
   - Weighted: edges have costs/weights (like distance, time, etc.)  
   - Unweighted: all edges treated equally

3. **Cyclic** or **Acyclic**  
   - Cyclic: contains cycles  
   - Acyclic: no cycles (e.g., DAG ‚Äì Directed Acyclic Graph)

4. **Connected** or **Disconnected**  
   - Connected: there's a path between every pair of vertices  
   - Disconnected: not all nodes are reachable from each other

---

### üìä Graph Representations

1. **Adjacency Matrix**  
   - 2D array where `matrix[i][j] = 1` if there‚Äôs an edge from `i` to `j`
   - Pros: Fast lookup  
   - Cons: Space = O(V¬≤)

2. **Adjacency List**  
   - Array of lists: each index `i` stores all neighbors of vertex `i`  
   - Pros: Space = O(V + E), better for sparse graphs

---

### üîç Breadth-First Search (BFS)

- **Level-order traversal** of a graph.
- Uses a **queue (FIFO)**.
- Good for **finding shortest path (unweighted)**, or all nodes reachable from a start node.

#### Pseudocode:
```cpp
BFS(Graph, start):
  create empty queue Q
  mark start as visited
  enqueue start into Q

  while Q is not empty:
    node = Q.dequeue()
    for each neighbor of node:
      if not visited:
        mark visited
        enqueue neighbor
```

#### Example:

Graph:
```
A -- B -- C
|         |
D ------- E
```

**BFS from A** ‚Üí `A, B, D, C, E`

---

### üîé Depth-First Search (DFS)

- **Explore as deep as possible** before backtracking.
- Uses **stack** (explicit or via recursion).
- Good for path-finding, cycle detection, topological sort, etc.

#### Pseudocode (recursive):
```cpp
DFS(Graph, node):
  mark node as visited
  for each neighbor:
    if not visited:
      DFS(neighbor)
```

**DFS from A** ‚Üí `A, B, C, E, D` *(depends on adjacency order)*

---

### üîÅ BFS vs. DFS

| Feature            | BFS                         | DFS                         |
|--------------------|-----------------------------|-----------------------------|
| Uses               | Queue                       | Stack / Recursion           |
| Traversal Order    | Level by level              | Deep before wide            |
| Space              | O(V)                        | O(V)                        |
| Shortest Path      | ‚úÖ Yes (unweighted graphs)   | ‚ùå No                        |
| Cycle Detection    | ‚úÖ (with parent tracking)    | ‚úÖ                          |
| Complete?          | Yes                         | Yes                         |

---

### üìö Common Applications

- **BFS**: Shortest path (unweighted), social networks, web crawlers  
- **DFS**: Topological sort, detecting cycles, solving puzzles (e.g., maze), connected components

---

Awesome ‚Äî you‚Äôre covering some of the most powerful graph algorithms! Let‚Äôs go over:

1. ‚úÖ **Dijkstra‚Äôs Algorithm** (for shortest paths)  
2. ‚úÖ **Minimum Spanning Tree (MST)**  
3. ‚úÖ **Prim‚Äôs Algorithm** (to find MST)

---

## üöÄ Dijkstra‚Äôs Algorithm

**Purpose**: Find the **shortest path** from a **source node** to **all other nodes** in a **weighted graph with non-negative edge weights**.

---

### üß† How it Works:

- Uses a **priority queue** (min-heap).
- Maintains a **distance array** (initially all ‚àû, source = 0).
- Always chooses the **closest unvisited node**.
- Updates distances to its neighbors (relaxation).
- Once a node is visited, its shortest path is final.

---

### üßÆ Pseudocode (simplified):

```cpp
function Dijkstra(Graph, source):
    dist[] = {‚àû, ‚àû, ..., ‚àû}
    dist[source] = 0
    minHeap = { (0, source) }

    while minHeap not empty:
        (d, u) = minHeap.pop()
        for each neighbor v of u:
            if dist[v] > dist[u] + weight(u, v):
                dist[v] = dist[u] + weight(u, v)
                minHeap.push((dist[v], v))
```

---

### Example:

Graph:
```
     (2)
 A ------- B
 |         |
(1)       (3)
 |         |
 C ------- D
     (1)
```

- Source = A  
- Dijkstra gives shortest paths:
  - A ‚Üí A: 0
  - A ‚Üí C: 1
  - A ‚Üí D: 2
  - A ‚Üí B: 2

---

## üå≤ Minimum Spanning Tree (MST)

**Goal**: Connect **all vertices** of a weighted undirected graph with **minimum total edge weight** and **no cycles**.

- MST has exactly **V-1 edges**
- **Multiple MSTs** may exist if weights are not unique.

---

## üîß Prim‚Äôs Algorithm

**Purpose**: Find the **MST** using a **greedy** approach.

---

### üß† How it Works:

- Start with any node.
- Grow the tree by **adding the smallest edge** connecting the tree to a new node.
- Use a **min-heap** to choose the smallest edge.

---

### üßÆ Pseudocode (simplified):

```cpp
function Prim(Graph, start):
    visited[] = false
    minHeap = { (0, start) }
    totalCost = 0

    while minHeap not empty:
        (cost, u) = minHeap.pop()
        if visited[u]: continue
        visited[u] = true
        totalCost += cost

        for each (v, weight) adjacent to u:
            if not visited[v]:
                minHeap.push((weight, v))
```

---

### Prim vs Dijkstra

| Feature             | Prim's                        | Dijkstra's                      |
|---------------------|-------------------------------|----------------------------------|
| Goal                | Minimum Spanning Tree (MST)   | Shortest paths from a source     |
| Graph Type          | Undirected, weighted          | Directed or undirected, weighted (no negative) |
| Strategy            | Greedy                        | Greedy with relaxation           |
| Starts With         | Any node                      | Source node                      |

---

### MST Example:

Graph:
```
   (2)
A------B
| \    |
1  3   4
|   \  |
C------D
    (5)
```

**Prim‚Äôs MST** might include:
- A‚ÄîC (1)
- A‚ÄîB (2)
- C‚ÄîD (3)

Total cost: **1 + 2 + 3 = 6**

---

‚úÖ Let me know if you want to code one of these in C++, trace an example by hand, or look at Kruskal‚Äôs Algorithm next!
